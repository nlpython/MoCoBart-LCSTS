K = 1024
T = 0.07
adam_epsilon = 1e-08
alpha = 5
bart_lr = 0.0001
batch_size = 16
checkpoint_path = checkpoints/
chunk_nums = 30
content_max_len = 152
data_dir = data/
epochs = 2
eval_interval = 2500
generate_max_len = 70
gradient_accumulation_steps = 4
json_path = bart-base-chinese/config.json
log_interval = 50
log_path = logs/
m = 0.999
max_clip_norm = 1.0
max_len = 184
mlp = False
pooling = last-avg
pretrained_model_path = bart-base-chinese/
repetition_penalty = 1.2
seed = 2020
summary_max_len = 32
top_k = 5
top_p = 0.95
vocab_path = bart-base-chinese/
warmup_steps = 10000
weight_decay = 0.01
